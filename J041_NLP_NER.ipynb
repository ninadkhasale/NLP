{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 1024\nOUTPUT_DIR = \"output\"","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:09:53.195233Z","iopub.execute_input":"2024-09-15T18:09:53.195817Z","iopub.status.idle":"2024-09-15T18:09:53.206472Z","shell.execute_reply.started":"2024-09-15T18:09:53.195788Z","shell.execute_reply":"2024-09-15T18:09:53.205583Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-15T18:09:53.226085Z","iopub.execute_input":"2024-09-15T18:09:53.226358Z","iopub.status.idle":"2024-09-15T18:10:10.309519Z","shell.execute_reply.started":"2024-09-15T18:09:53.226337Z","shell.execute_reply":"2024-09-15T18:10:10.308470Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\n\nfrom seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:10.311674Z","iopub.execute_input":"2024-09-15T18:10:10.311973Z","iopub.status.idle":"2024-09-15T18:10:30.909895Z","shell.execute_reply.started":"2024-09-15T18:10:10.311945Z","shell.execute_reply":"2024-09-15T18:10:30.908878Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-09-15 18:10:20.891860: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-15 18:10:20.891963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-15 18:10:21.019187: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data (pre)processing","metadata":{}},{"cell_type":"markdown","source":"## Get data","metadata":{}},{"cell_type":"code","source":"train_data = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/train.json'))","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:30.911086Z","iopub.execute_input":"2024-09-15T18:10:30.911887Z","iopub.status.idle":"2024-09-15T18:10:33.476607Z","shell.execute_reply.started":"2024-09-15T18:10:30.911849Z","shell.execute_reply":"2024-09-15T18:10:33.475752Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:33.477698Z","iopub.execute_input":"2024-09-15T18:10:33.477983Z","iopub.status.idle":"2024-09-15T18:10:33.484848Z","shell.execute_reply.started":"2024-09-15T18:10:33.477958Z","shell.execute_reply":"2024-09-15T18:10:33.483856Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"6807"},"metadata":{}}]},{"cell_type":"markdown","source":"## Mapping","metadata":{}},{"cell_type":"code","source":"# Map labels to ids\nall_labels = sorted(list(set(chain(*[x[\"labels\"] for x in train_data]))))\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}\n\ntarget = [item for item in all_labels if item != 'O']\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:33.487664Z","iopub.execute_input":"2024-09-15T18:10:33.487946Z","iopub.status.idle":"2024-09-15T18:10:33.564982Z","shell.execute_reply.started":"2024-09-15T18:10:33.487901Z","shell.execute_reply":"2024-09-15T18:10:33.564004Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def rebuild_text(data):\n    \n    text, labels = [], []\n    \n    for tok, lab, ws in zip(\n        data[\"tokens\"], data[\"provided_labels\"], data[\"trailing_whitespace\"]\n    ):\n        # append each token to the reconstructed text and the label for each token's character\n        text.append(tok)\n        labels.extend([lab] * len(tok))\n        \n        # add space in text if whitespace and label \"O\"\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            \n    return text, labels","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:33.566250Z","iopub.execute_input":"2024-09-15T18:10:33.566601Z","iopub.status.idle":"2024-09-15T18:10:33.574700Z","shell.execute_reply.started":"2024-09-15T18:10:33.566570Z","shell.execute_reply":"2024-09-15T18:10:33.573906Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Prepare data to be fed to the model & attribute labels to new token format\ndef tokenize(data, tokenizer, label2id, max_length):\n    \n    text, labels = rebuild_text(data)\n    text = \"\".join(text)\n    labels = np.array(labels)\n    token_labels = []\n    \n    # returns a dictionary-like object containing tokenized inputs and offsets mapping (represents the mapping between the tokens and their corresponding positions in the original text)\n    tokenized = tokenizer(text, return_offsets_mapping=True, max_length=max_length)\n    \n    for start_idx, end_idx in tokenized.offset_mapping:\n        \n        # if CLS tokens\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n            \n        # if token starts with ws\n        if text[start_idx].isspace():\n            start_idx += 1\n            \n        token_labels.append(label2id[labels[start_idx]])\n        \n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:33.575887Z","iopub.execute_input":"2024-09-15T18:10:33.576145Z","iopub.status.idle":"2024-09-15T18:10:33.584024Z","shell.execute_reply.started":"2024-09-15T18:10:33.576122Z","shell.execute_reply":"2024-09-15T18:10:33.583241Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:33.585169Z","iopub.execute_input":"2024-09-15T18:10:33.585521Z","iopub.status.idle":"2024-09-15T18:10:35.891572Z","shell.execute_reply.started":"2024-09-15T18:10:33.585491Z","shell.execute_reply":"2024-09-15T18:10:35.890662Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c9ffe3416b4a70aa916cf50767167e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af507968b06434a9d7df34050c5cafa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97bb2a9da2b42d0a3c9bb7c59fef704"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in train_data],\n    \"document\": [str(x[\"document\"]) for x in train_data],\n    \"tokens\": [x[\"tokens\"] for x in train_data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in train_data],\n    \"provided_labels\": [x[\"labels\"] for x in train_data],\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:35.892953Z","iopub.execute_input":"2024-09-15T18:10:35.893297Z","iopub.status.idle":"2024-09-15T18:10:37.389989Z","shell.execute_reply.started":"2024-09-15T18:10:35.893264Z","shell.execute_reply":"2024-09-15T18:10:37.388978Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# tokenize each row in the dataset\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\":tokenizer, \"label2id\":label2id, \"max_length\":TRAINING_MAX_LENGTH}, num_proc=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:10:37.391215Z","iopub.execute_input":"2024-09-15T18:10:37.391525Z","iopub.status.idle":"2024-09-15T18:12:00.580246Z","shell.execute_reply.started":"2024-09-15T18:10:37.391499Z","shell.execute_reply":"2024-09-15T18:12:00.579197Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"    ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/2269 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a13c0ee1a6f409bb24266a3f977ecf7"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/2269 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c1d7dd8b48143718571c6c9b4750209"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/2269 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461f785f8c634d7c85633268ab7ef3fb"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compare tokens and labels for original dataset and new tokenization\nx = ds[0]\n\nfor t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n    if l != \"O\":\n        print((t,l))\n\nprint(\"*\"*100)\n\nfor t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n    if id2label[l] != \"O\":\n        print((t,id2label[l]))","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:12:00.581649Z","iopub.execute_input":"2024-09-15T18:12:00.581916Z","iopub.status.idle":"2024-09-15T18:12:00.598579Z","shell.execute_reply.started":"2024-09-15T18:12:00.581889Z","shell.execute_reply":"2024-09-15T18:12:00.597674Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n****************************************************************************************************\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('▁S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('▁S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('▁S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_metrics(p, all_labels):\n    # p is a tuple containing preds and true labels\n    predictions, labels = p\n    # preds are in form of probs for each label for each token => we take the highest one\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove special tokens from preds and labels\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    # Compute metrics using sklearn and own formula\n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    # Store metrics and return\n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:12:00.599926Z","iopub.execute_input":"2024-09-15T18:12:00.600213Z","iopub.status.idle":"2024-09-15T18:12:00.611069Z","shell.execute_reply.started":"2024-09-15T18:12:00.600188Z","shell.execute_reply":"2024-09-15T18:12:00.610279Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:12:00.612207Z","iopub.execute_input":"2024-09-15T18:12:00.612590Z","iopub.status.idle":"2024-09-15T18:12:04.223472Z","shell.execute_reply.started":"2024-09-15T18:12:00.612560Z","shell.execute_reply":"2024-09-15T18:12:04.222739Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"078685784efc4d43bc2762878447f494"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creates a collator object (tailored for token classification tasks)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:12:04.228160Z","iopub.execute_input":"2024-09-15T18:12:04.228428Z","iopub.status.idle":"2024-09-15T18:12:04.232756Z","shell.execute_reply.started":"2024-09-15T18:12:04.228404Z","shell.execute_reply":"2024-09-15T18:12:04.231735Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Define training arguments\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    evaluation_strategy=\"no\",\n    do_eval=False,\n    save_total_limit=1,\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:12:04.233794Z","iopub.execute_input":"2024-09-15T18:12:04.234074Z","iopub.status.idle":"2024-09-15T18:12:04.292052Z","shell.execute_reply.started":"2024-09-15T18:12:04.234049Z","shell.execute_reply":"2024-09-15T18:12:04.291292Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define trainer object (responsible for orchestrating the training process)\ntrainer = Trainer(\n    model=model, \n    args=args, \n    train_dataset=ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:12:04.293019Z","iopub.execute_input":"2024-09-15T18:12:04.293266Z","iopub.status.idle":"2024-09-15T18:12:04.628176Z","shell.execute_reply.started":"2024-09-15T18:12:04.293244Z","shell.execute_reply":"2024-09-15T18:12:04.627398Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:12:04.629314Z","iopub.execute_input":"2024-09-15T18:12:04.629654Z","iopub.status.idle":"2024-09-15T18:34:35.110225Z","shell.execute_reply.started":"2024-09-15T18:12:04.629623Z","shell.execute_reply":"2024-09-15T18:34:35.109310Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='851' max='851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [851/851 22:27, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.279100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.449200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.057800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.006900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.012300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.008300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.008100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.006600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.001500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 17min 54s, sys: 4min 35s, total: 22min 30s\nWall time: 22min 30s\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=851, training_loss=0.11622859703674562, metrics={'train_runtime': 1349.9694, 'train_samples_per_second': 5.042, 'train_steps_per_second': 0.63, 'total_flos': 3161498795311008.0, 'train_loss': 0.11622859703674562, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"deberta3base_1024\")\ntokenizer.save_pretrained(\"deberta3base_1024\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:34:35.111542Z","iopub.execute_input":"2024-09-15T18:34:35.111910Z","iopub.status.idle":"2024-09-15T18:34:36.833979Z","shell.execute_reply.started":"2024-09-15T18:34:35.111876Z","shell.execute_reply":"2024-09-15T18:34:36.832931Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('deberta3base_1024/tokenizer_config.json',\n 'deberta3base_1024/special_tokens_map.json',\n 'deberta3base_1024/spm.model',\n 'deberta3base_1024/added_tokens.json',\n 'deberta3base_1024/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH = 2048\nmodel_path = '/kaggle/working/deberta3base_1024'","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:35:53.683004Z","iopub.execute_input":"2024-09-15T18:35:53.683740Z","iopub.status.idle":"2024-09-15T18:35:53.687750Z","shell.execute_reply.started":"2024-09-15T18:35:53.683705Z","shell.execute_reply":"2024-09-15T18:35:53.686762Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom itertools import chain\nfrom pathlib import Path\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:35:56.826716Z","iopub.execute_input":"2024-09-15T18:35:56.827094Z","iopub.status.idle":"2024-09-15T18:35:56.832620Z","shell.execute_reply.started":"2024-09-15T18:35:56.827062Z","shell.execute_reply":"2024-09-15T18:35:56.831597Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"test_data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:18.390609Z","iopub.execute_input":"2024-09-15T18:36:18.391348Z","iopub.status.idle":"2024-09-15T18:36:18.399268Z","shell.execute_reply.started":"2024-09-15T18:36:18.391318Z","shell.execute_reply":"2024-09-15T18:36:18.398442Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def tokenize(data, tokenizer):\n    \n    text, token_map = [], []\n    idx = 0\n    \n    for tok, ws in zip(data[\"tokens\"], data[\"trailing_whitespace\"]):\n        \n        text.append(tok)\n        token_map.extend([idx] * len(tok))\n        \n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n        \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    \n        \n    return {**tokenized, \"token_map\": token_map}","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:19.491671Z","iopub.execute_input":"2024-09-15T18:36:19.492328Z","iopub.status.idle":"2024-09-15T18:36:19.499032Z","shell.execute_reply.started":"2024-09-15T18:36:19.492293Z","shell.execute_reply":"2024-09-15T18:36:19.498145Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"ds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in test_data],\n    \"document\": [x[\"document\"] for x in test_data],\n    \"tokens\": [x[\"tokens\"] for x in test_data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in test_data],\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:21.384186Z","iopub.execute_input":"2024-09-15T18:36:21.385089Z","iopub.status.idle":"2024-09-15T18:36:21.399655Z","shell.execute_reply.started":"2024-09-15T18:36:21.385043Z","shell.execute_reply":"2024-09-15T18:36:21.398682Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:22.460869Z","iopub.execute_input":"2024-09-15T18:36:22.461238Z","iopub.status.idle":"2024-09-15T18:36:22.728650Z","shell.execute_reply.started":"2024-09-15T18:36:22.461207Z","shell.execute_reply":"2024-09-15T18:36:22.727620Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:23.154421Z","iopub.execute_input":"2024-09-15T18:36:23.155094Z","iopub.status.idle":"2024-09-15T18:36:23.969617Z","shell.execute_reply.started":"2024-09-15T18:36:23.155059Z","shell.execute_reply":"2024-09-15T18:36:23.968630Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"   ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/5 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ef03819b1340449bf7c061609584e0"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/5 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"205f59f9789d456b8687211581e1e61d"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(model_path)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=1, \n    report_to=\"none\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:25.455862Z","iopub.execute_input":"2024-09-15T18:36:25.456644Z","iopub.status.idle":"2024-09-15T18:36:25.822270Z","shell.execute_reply.started":"2024-09-15T18:36:25.456594Z","shell.execute_reply":"2024-09-15T18:36:25.821515Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model, \n    args=args, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:27.319243Z","iopub.execute_input":"2024-09-15T18:36:27.319902Z","iopub.status.idle":"2024-09-15T18:36:27.526773Z","shell.execute_reply.started":"2024-09-15T18:36:27.319869Z","shell.execute_reply":"2024-09-15T18:36:27.525846Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = trainer.predict(ds).predictions\npred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:28.732686Z","iopub.execute_input":"2024-09-15T18:36:28.733021Z","iopub.status.idle":"2024-09-15T18:36:29.467574Z","shell.execute_reply.started":"2024-09-15T18:36:28.732995Z","shell.execute_reply":"2024-09-15T18:36:29.466857Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"config = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:30.352380Z","iopub.execute_input":"2024-09-15T18:36:30.353195Z","iopub.status.idle":"2024-09-15T18:36:30.358317Z","shell.execute_reply.started":"2024-09-15T18:36:30.353160Z","shell.execute_reply":"2024-09-15T18:36:30.357348Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"preds = predictions.argmax(-1)\npreds_without_O = pred_softmax[:,:,:12].argmax(-1)\nO_preds = pred_softmax[:,:,12]","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:31.438886Z","iopub.execute_input":"2024-09-15T18:36:31.439596Z","iopub.status.idle":"2024-09-15T18:36:31.446772Z","shell.execute_reply.started":"2024-09-15T18:36:31.439563Z","shell.execute_reply":"2024-09-15T18:36:31.445925Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"threshold = 0.9\npreds_final = np.where(O_preds < threshold, preds_without_O, preds)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:32.660896Z","iopub.execute_input":"2024-09-15T18:36:32.661258Z","iopub.status.idle":"2024-09-15T18:36:32.666659Z","shell.execute_reply.started":"2024-09-15T18:36:32.661226Z","shell.execute_reply":"2024-09-15T18:36:32.665742Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"triplets = []\ndocument, token, label, token_str = [], [], [], []","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:33.762337Z","iopub.execute_input":"2024-09-15T18:36:33.762758Z","iopub.status.idle":"2024-09-15T18:36:33.767491Z","shell.execute_reply.started":"2024-09-15T18:36:33.762726Z","shell.execute_reply":"2024-09-15T18:36:33.766454Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[str(token_pred)]\n\n        if start_idx + end_idx == 0: continue\n\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # ignore \"\\n\\n\"\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        if start_idx >= len(token_map): break\n        \n        token_id = token_map[start_idx]\n\n        # ignore \"O\" predictions and whitespace preds\n        if label_pred != \"O\" and token_id != -1:\n            triplet = (label_pred, token_id, tokens[token_id])\n\n            if triplet not in triplets:\n                document.append(doc)\n                token.append(token_id)\n                label.append(label_pred)\n                token_str.append(tokens[token_id])\n                triplets.append(triplet)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:36.078360Z","iopub.execute_input":"2024-09-15T18:36:36.079015Z","iopub.status.idle":"2024-09-15T18:36:36.175664Z","shell.execute_reply.started":"2024-09-15T18:36:36.078980Z","shell.execute_reply":"2024-09-15T18:36:36.174896Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    \"document\": document,\n    \"token\": token,\n    \"label\": label,\n    \"token_str\": token_str\n})\ndf[\"row_id\"] = list(range(len(df)))\ndisplay(df.head(100))","metadata":{"execution":{"iopub.status.busy":"2024-09-15T18:36:37.275704Z","iopub.execute_input":"2024-09-15T18:36:37.276293Z","iopub.status.idle":"2024-09-15T18:36:37.304746Z","shell.execute_reply.started":"2024-09-15T18:36:37.276255Z","shell.execute_reply":"2024-09-15T18:36:37.303794Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"    document  token           label  \\\n0          7      9  B-NAME_STUDENT   \n1          7     10  I-NAME_STUDENT   \n2          7    482  B-NAME_STUDENT   \n3          7    483  I-NAME_STUDENT   \n4          7    741  B-NAME_STUDENT   \n5          7    742  I-NAME_STUDENT   \n6         10      0  B-NAME_STUDENT   \n7         10      1  I-NAME_STUDENT   \n8         10    464  B-NAME_STUDENT   \n9         10    465  I-NAME_STUDENT   \n10        16      4  B-NAME_STUDENT   \n11        16      5  I-NAME_STUDENT   \n12        20      5  B-NAME_STUDENT   \n13        20      6  I-NAME_STUDENT   \n14        20      8  I-NAME_STUDENT   \n15        56     12  B-NAME_STUDENT   \n16        56     13  I-NAME_STUDENT   \n17        86      6  B-NAME_STUDENT   \n18        86      7  I-NAME_STUDENT   \n19        93      0  B-NAME_STUDENT   \n20        93      1  I-NAME_STUDENT   \n21       104      7  B-NAME_STUDENT   \n22       104      8  B-NAME_STUDENT   \n23       104      9  I-NAME_STUDENT   \n24       112      5  B-NAME_STUDENT   \n25       112      6  I-NAME_STUDENT   \n26       123     32  B-NAME_STUDENT   \n27       123     33  I-NAME_STUDENT   \n28       123     38  B-NAME_STUDENT   \n29       123     38  I-NAME_STUDENT   \n30       123    223  B-NAME_STUDENT   \n31       123    223  I-NAME_STUDENT   \n32       123    224  B-NAME_STUDENT   \n33       123    490  B-NAME_STUDENT   \n34       123    491  B-NAME_STUDENT   \n35       123    491  I-NAME_STUDENT   \n36       123   1500  B-NAME_STUDENT   \n37       123   1509  B-URL_PERSONAL   \n38       123   1530  B-NAME_STUDENT   \n39       123   1531  B-NAME_STUDENT   \n40       123   1556  B-NAME_STUDENT   \n41       123   1557  B-NAME_STUDENT   \n42       123   1575  B-URL_PERSONAL   \n43       123   1575  B-NAME_STUDENT   \n44       123   1575        B-ID_NUM   \n45       123   1581  B-NAME_STUDENT   \n46       123   1591  B-URL_PERSONAL   \n47       123   1602  B-NAME_STUDENT   \n48       123   1648  B-URL_PERSONAL   \n49       123   1690  B-URL_PERSONAL   \n50       123   1691  B-URL_PERSONAL   \n\n                                            token_str  row_id  \n0                                            Nathalie       0  \n1                                               Sylla       1  \n2                                            Nathalie       2  \n3                                               Sylla       3  \n4                                            Nathalie       4  \n5                                               Sylla       5  \n6                                               Diego       6  \n7                                             Estrada       7  \n8                                               Diego       8  \n9                                             Estrada       9  \n10                                           Gilberto      10  \n11                                             Gamboa      11  \n12                                              Sindy      12  \n13                                             Samaca      13  \n14                                              Gitam      14  \n15                                             Nadine      15  \n16                                               Born      16  \n17                                             Eladio      17  \n18                                              Amaya      18  \n19                                             Silvia      19  \n20                                         Villalobos      20  \n21                                                 Dr      21  \n22                                              Sakir      22  \n23                                              Ahmad      23  \n24                                          Francisco      24  \n25                                           Ferreira      25  \n26                                            Stefano      26  \n27                                             Lovato      27  \n28                                         Sathyabama      28  \n29                                         Sathyabama      29  \n30                                       Gerashchenko      30  \n31                                       Gerashchenko      31  \n32                                               Igor      32  \n33                                          Alexander      33  \n34                                            Shmakov      34  \n35                                            Shmakov      35  \n36                                         Kazantseva      36  \n37      https://cyberleninka.ru/article/n/stremlenie-      37  \n38                                       Gerashchenko      38  \n39                                               I.G.      39  \n40                                            Shmakov      40  \n41                                               A.V.      41  \n42         https://cyberleninka.ru/article/n/14398333      42  \n43         https://cyberleninka.ru/article/n/14398333      43  \n44         https://cyberleninka.ru/article/n/14398333      44  \n45                                         Kazantseva      45  \n46  https://cyberleninka.ru/article/n/stremlenie-k...      46  \n47                                           Andreoni      47  \n48  https://econweb.ucsd.edu/~jandreon/WorkingPape...      48  \n49                           https://www.melessa.uni-      49  \n50  muenchen.de/team/vorstandssprecher/schmidt/pub...      50  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Samaca</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>20</td>\n      <td>8</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gitam</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nadine</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Born</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>104</td>\n      <td>7</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Dr</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Stefano</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Lovato</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>123</td>\n      <td>38</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sathyabama</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>123</td>\n      <td>38</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sathyabama</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>123</td>\n      <td>223</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gerashchenko</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>123</td>\n      <td>223</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gerashchenko</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>123</td>\n      <td>224</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Igor</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>123</td>\n      <td>490</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Alexander</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>123</td>\n      <td>491</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Shmakov</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>123</td>\n      <td>491</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Shmakov</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>123</td>\n      <td>1500</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Kazantseva</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>123</td>\n      <td>1509</td>\n      <td>B-URL_PERSONAL</td>\n      <td>https://cyberleninka.ru/article/n/stremlenie-</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>123</td>\n      <td>1530</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gerashchenko</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>123</td>\n      <td>1531</td>\n      <td>B-NAME_STUDENT</td>\n      <td>I.G.</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>123</td>\n      <td>1556</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Shmakov</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>123</td>\n      <td>1557</td>\n      <td>B-NAME_STUDENT</td>\n      <td>A.V.</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>123</td>\n      <td>1575</td>\n      <td>B-URL_PERSONAL</td>\n      <td>https://cyberleninka.ru/article/n/14398333</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>123</td>\n      <td>1575</td>\n      <td>B-NAME_STUDENT</td>\n      <td>https://cyberleninka.ru/article/n/14398333</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>123</td>\n      <td>1575</td>\n      <td>B-ID_NUM</td>\n      <td>https://cyberleninka.ru/article/n/14398333</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>123</td>\n      <td>1581</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Kazantseva</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>123</td>\n      <td>1591</td>\n      <td>B-URL_PERSONAL</td>\n      <td>https://cyberleninka.ru/article/n/stremlenie-k...</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>123</td>\n      <td>1602</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Andreoni</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>123</td>\n      <td>1648</td>\n      <td>B-URL_PERSONAL</td>\n      <td>https://econweb.ucsd.edu/~jandreon/WorkingPape...</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>123</td>\n      <td>1690</td>\n      <td>B-URL_PERSONAL</td>\n      <td>https://www.melessa.uni-</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>123</td>\n      <td>1691</td>\n      <td>B-URL_PERSONAL</td>\n      <td>muenchen.de/team/vorstandssprecher/schmidt/pub...</td>\n      <td>50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}