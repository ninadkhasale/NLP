{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unDnnKzYbWpg",
        "outputId": "24d64ef3-78ed-40d2-b142-103cfee08883"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5jY5y6-dph3",
        "outputId": "0208683e-f5a0-4bb3-dd44-c5c5ef4e043d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M4QWMVRzdpkp"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/My Drive/Colab Notebooks/IMDB_Dataset.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gMSYj429eLGa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "grDuyplzcLMn"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    text = word_tokenize(text)\n",
        "   # text = [word for word in text if word not in stopwords.words('english')]\n",
        "    return text\n",
        "\n",
        "df['review'] = df['review'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9-tYZIPg1Qv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "26d82v5rg1TR"
      },
      "outputs": [],
      "source": [
        "# Encoding labels\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ET-fB6T3cLPn"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-5sBbmMLmcoN"
      },
      "outputs": [],
      "source": [
        "glove_file_path = '/content/drive/My Drive/Colab Notebooks/glove.6B.100d.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywwYg1l9hF4j",
        "outputId": "b4eb5cfd-3169-405a-ab8d-d8cf45facef7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400000it [00:14, 27169.17it/s]\n"
          ]
        }
      ],
      "source": [
        "def load_glove_embeddings(glove_file=glove_file_path):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NrGMTvy1hLMn"
      },
      "outputs": [],
      "source": [
        "# Prepare embedding matrix and word2idx mapping\n",
        "vocab = set([word for text in X_train for word in text])\n",
        "word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
        "embedding_matrix = np.zeros((len(vocab) + 1, 100))\n",
        "\n",
        "for word, idx in word2idx.items():\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[idx] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kFUar1kQmyaW"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word2idx, max_len=100):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "        text_indices = [self.word2idx.get(word, 0) for word in text[:self.max_len]]\n",
        "        padding = [0] * (self.max_len - len(text_indices))\n",
        "        text_indices = text_indices + padding\n",
        "        return torch.tensor(text_indices, dtype=torch.long), torch.tensor(label, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7usBVSzsmy4Y"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_dataset = IMDBDataset(X_train, y_train, word2idx)\n",
        "test_dataset = IMDBDataset(X_test, y_test, word2idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XGSKgwtSmy7U"
      },
      "outputs": [],
      "source": [
        "# Vanilla RNN Model\n",
        "class VanillaRNNModel(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim=128, output_dim=1):\n",
        "        super(VanillaRNNModel, self).__init__()\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, hidden = self.rnn(embedded)\n",
        "        out = self.fc(hidden.squeeze(0))\n",
        "        return self.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sAe3m9qwos4o"
      },
      "outputs": [],
      "source": [
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim=128, output_dim=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        out = self.fc(hidden.squeeze(0))\n",
        "        return self.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CRWBcilyowNx"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for texts, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XAd0Cy0co08A"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            outputs = model(texts)\n",
        "            preds = outputs.squeeze().round()\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(preds.numpy())\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZgaX-5ro5s1",
        "outputId": "2715ff14-eb85-467f-c575-a3b765ba63d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 0.6919156236171723\n",
            "Epoch 2/5, Loss: 0.6688818548202514\n"
          ]
        }
      ],
      "source": [
        "# Task 1: Vanilla RNN with GloVe\n",
        "vanilla_rnn_model = VanillaRNNModel(embedding_matrix)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(vanilla_rnn_model.parameters(), lr=0.001)\n",
        "train_model(vanilla_rnn_model, train_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(vanilla_rnn_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlBrEUyKo99y"
      },
      "outputs": [],
      "source": [
        "# Task 2: LSTM with GloVe\n",
        "lstm_model = LSTMModel(embedding_matrix)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "train_model(lstm_model, train_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(lstm_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-TuHt2QrE-a"
      },
      "outputs": [],
      "source": [
        "# Task 3: Vanilla RNN with on-the-fly embeddings\n",
        "vanilla_rnn_onthefly = VanillaRNNModel(embedding_matrix=None)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(vanilla_rnn_onthefly.parameters(), lr=0.001)\n",
        "train_model(vanilla_rnn_onthefly, train_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(vanilla_rnn_onthefly, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNuK1KWjrFBl"
      },
      "outputs": [],
      "source": [
        "# Task 3: LSTM with on-the-fly embeddings\n",
        "lstm_onthefly = LSTMModel(embedding_matrix=None)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(lstm_onthefly.parameters(), lr=0.001)\n",
        "train_model(lstm_onthefly, train_loader, criterion, optimizer, epochs=5)\n",
        "evaluate_model(lstm_onthefly, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
