{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.7/43.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/9.5 MB 37.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.5 MB 49.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.4/9.5 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 55.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 55.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 55.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 30.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "   ---------------------------------------- 0.0/417.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 417.5/417.5 kB 25.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.0/286.0 kB 17.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.9/2.2 MB 60.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 21.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.6 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:08.079707Z",
     "iopub.status.busy": "2024-09-01T15:32:08.078770Z",
     "iopub.status.idle": "2024-09-01T15:32:08.091510Z",
     "shell.execute_reply": "2024-09-01T15:32:08.090378Z",
     "shell.execute_reply.started": "2024-09-01T15:32:08.079641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n",
      "/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:08.094222Z",
     "iopub.status.busy": "2024-09-01T15:32:08.093788Z",
     "iopub.status.idle": "2024-09-01T15:32:08.992812Z",
     "shell.execute_reply": "2024-09-01T15:32:08.991618Z",
     "shell.execute_reply.started": "2024-09-01T15:32:08.094173Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('/kaggle/input/stanford-question-answering-dataset/train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:08.995537Z",
     "iopub.status.busy": "2024-09-01T15:32:08.995067Z",
     "iopub.status.idle": "2024-09-01T15:32:09.087265Z",
     "shell.execute_reply": "2024-09-01T15:32:09.086026Z",
     "shell.execute_reply.started": "2024-09-01T15:32:08.995485Z"
    }
   },
   "outputs": [],
   "source": [
    "listt = []\n",
    "for i in data['data']:\n",
    "    d = {}\n",
    "    for para in i['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                d['answer'] = qa['answers'][0]['answer_start']\n",
    "                d['text'] = qa['answers'][0]['text']\n",
    "                d['question'] = qa['question']\n",
    "                d['context'] = context\n",
    "    listt.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.089103Z",
     "iopub.status.busy": "2024-09-01T15:32:09.088668Z",
     "iopub.status.idle": "2024-09-01T15:32:09.098200Z",
     "shell.execute_reply": "2024-09-01T15:32:09.096978Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.089038Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(listt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.102048Z",
     "iopub.status.busy": "2024-09-01T15:32:09.101603Z",
     "iopub.status.idle": "2024-09-01T15:32:09.130018Z",
     "shell.execute_reply": "2024-09-01T15:32:09.128833Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.101995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1232</td>\n",
       "      <td>Jim Wetherbee</td>\n",
       "      <td>Which notable astronaut is known to have atten...</td>\n",
       "      <td>Notre Dame alumni work in various fields. Alum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>533</td>\n",
       "      <td>Salma Hayek and Frida Giannini</td>\n",
       "      <td>Who did Beyoncé work with in 2013 on the Chime...</td>\n",
       "      <td>In December, Beyoncé along with a variety of o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>926</td>\n",
       "      <td>Great Falls, Lewistown, Cut Bank and Glasgow</td>\n",
       "      <td>Where were air bases built in Montana?</td>\n",
       "      <td>When the U.S. entered World War II on December...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>995</td>\n",
       "      <td>humanizing a devalued group</td>\n",
       "      <td>What is one preventive effort in circumventing...</td>\n",
       "      <td>Other authors have focused on the structural c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>763</td>\n",
       "      <td>Allan Coukell,</td>\n",
       "      <td>Who is a director at the Pew Charitable Trusts?</td>\n",
       "      <td>Possible improvements include clarification of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>456</td>\n",
       "      <td>establish a restaurant guest's identity and fo...</td>\n",
       "      <td>How could police help the owner when a restaur...</td>\n",
       "      <td>In contrast, the police are entitled to protec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>57</td>\n",
       "      <td>Ghazals and folk songs</td>\n",
       "      <td>What kind of music does Roshen Ara Begum perform?</td>\n",
       "      <td>For the popular taste however, light music, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>955</td>\n",
       "      <td>being bitten during a fight</td>\n",
       "      <td>How did tyrannosaurs become infected?</td>\n",
       "      <td>Evidence of infection in fossil remains is a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>404</td>\n",
       "      <td>poaching</td>\n",
       "      <td>What else is partly to blame for the declining...</td>\n",
       "      <td>In contrast, Botswana has recently been forced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     answer                                               text  \\\n",
       "0      1232                                      Jim Wetherbee   \n",
       "1       533                     Salma Hayek and Frida Giannini   \n",
       "2       926       Great Falls, Lewistown, Cut Bank and Glasgow   \n",
       "3       995                        humanizing a devalued group   \n",
       "4       763                                     Allan Coukell,   \n",
       "..      ...                                                ...   \n",
       "437     456  establish a restaurant guest's identity and fo...   \n",
       "438      57                             Ghazals and folk songs   \n",
       "439     955                        being bitten during a fight   \n",
       "440     404                                           poaching   \n",
       "441       0                        Kathmandu Metropolitan City   \n",
       "\n",
       "                                              question  \\\n",
       "0    Which notable astronaut is known to have atten...   \n",
       "1    Who did Beyoncé work with in 2013 on the Chime...   \n",
       "2               Where were air bases built in Montana?   \n",
       "3    What is one preventive effort in circumventing...   \n",
       "4      Who is a director at the Pew Charitable Trusts?   \n",
       "..                                                 ...   \n",
       "437  How could police help the owner when a restaur...   \n",
       "438  What kind of music does Roshen Ara Begum perform?   \n",
       "439              How did tyrannosaurs become infected?   \n",
       "440  What else is partly to blame for the declining...   \n",
       "441                      What is KMC an initialism of?   \n",
       "\n",
       "                                               context  \n",
       "0    Notre Dame alumni work in various fields. Alum...  \n",
       "1    In December, Beyoncé along with a variety of o...  \n",
       "2    When the U.S. entered World War II on December...  \n",
       "3    Other authors have focused on the structural c...  \n",
       "4    Possible improvements include clarification of...  \n",
       "..                                                 ...  \n",
       "437  In contrast, the police are entitled to protec...  \n",
       "438  For the popular taste however, light music, pa...  \n",
       "439  Evidence of infection in fossil remains is a s...  \n",
       "440  In contrast, Botswana has recently been forced...  \n",
       "441  Kathmandu Metropolitan City (KMC), in order to...  \n",
       "\n",
       "[442 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.131848Z",
     "iopub.status.busy": "2024-09-01T15:32:09.131396Z",
     "iopub.status.idle": "2024-09-01T15:32:09.138083Z",
     "shell.execute_reply": "2024-09-01T15:32:09.136993Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.131807Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"max_length\": 512,\n",
    "    \"model_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
    "    \n",
    "    \"output_dir\": \"./my-model\",\n",
    "    \"train_batch_size\": 64,\n",
    "    \"valid_batch_size\": 64,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"epochs\": 300,\n",
    "    \n",
    "    \"debug\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.140457Z",
     "iopub.status.busy": "2024-09-01T15:32:09.139690Z",
     "iopub.status.idle": "2024-09-01T15:32:09.896639Z",
     "shell.execute_reply": "2024-09-01T15:32:09.895324Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.140310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10260d55a1643f989a99214d8ae880b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/525 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9808d7dd74b4054bdbad21076cb73a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model_path\"])\n",
    "class TextDataset:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def preprocess_function(self,question, context, answer_start_char, answer_end_char):\n",
    "        inputs = tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            max_length=config[\"max_length\"],\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        offset = inputs.pop(\"offset_mapping\") \n",
    "        sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "        context_start, context_end = -1, -1\n",
    "\n",
    "\n",
    "        # Add logic to find the token indices for context start and context end using `sequence_ids``.\n",
    "        for index in range(len(sequence_ids)):\n",
    "            i = sequence_ids[index]\n",
    "            if context_start == -1:\n",
    "                if i == 1:\n",
    "                    context_start = index\n",
    "            else:\n",
    "                if i != 1:\n",
    "                    context_end = index          \n",
    "\n",
    "        context_offsets = offset[context_start: context_end]\n",
    "\n",
    "        # Create a mapping of character index to token index.\n",
    "        character_pos_to_token_pos = {}\n",
    "        for token_pos, (char_start, char_end) in enumerate(context_offsets):\n",
    "            token_pos1 = context_start + token_pos\n",
    "            for i in range(char_start, char_end+1):\n",
    "                character_pos_to_token_pos[i] = token_pos1\n",
    "\n",
    "        start_pos = character_pos_to_token_pos.get(answer_start_char, 0)\n",
    "        end_pos = character_pos_to_token_pos.get(\n",
    "            answer_end_char - 1, \n",
    "            0 if start_pos == 0 else config['max_length'] - 1\n",
    "        )\n",
    "\n",
    "        inputs[\"start_positions\"] = start_pos\n",
    "        inputs[\"end_positions\"] = end_pos\n",
    "\n",
    "        return inputs\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        question = row['question']\n",
    "        context = row['context']\n",
    "        answer_start = row['answer']\n",
    "        answer_end = answer_start + len(row['text'])\n",
    "        \n",
    "        return self.preprocess_function(question, context, answer_start,answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.898738Z",
     "iopub.status.busy": "2024-09-01T15:32:09.898384Z",
     "iopub.status.idle": "2024-09-01T15:32:09.903737Z",
     "shell.execute_reply": "2024-09-01T15:32:09.902210Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.898702Z"
    }
   },
   "outputs": [],
   "source": [
    "dff = TextDataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.905690Z",
     "iopub.status.busy": "2024-09-01T15:32:09.905349Z",
     "iopub.status.idle": "2024-09-01T15:32:09.917712Z",
     "shell.execute_reply": "2024-09-01T15:32:09.916599Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.905645Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(question, context, answer_start_char, answer_end_char):\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=config[\"max_length\"],\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    offset = inputs.pop(\"offset_mapping\") \n",
    "    sequence_ids = inputs.sequence_ids()\n",
    "    \n",
    "    context_start, context_end = -1, -1\n",
    "    \n",
    "\n",
    "    # Add logic to find the token indices for context start and context end using `sequence_ids``.\n",
    "    for index in range(len(sequence_ids)):\n",
    "        i = sequence_ids[index]\n",
    "        print(i)\n",
    "        if context_start == -1:\n",
    "            if i == 1:\n",
    "                context_start = index\n",
    "        else:\n",
    "            if i != 1:\n",
    "                context_end = index          \n",
    "    \n",
    "    context_offsets = offset[context_start: context_end]\n",
    "    \n",
    "    # Create a mapping of character index to token index.\n",
    "    character_pos_to_token_pos = {}\n",
    "    for token_pos, (char_start, char_end) in enumerate(context_offsets):\n",
    "        token_pos1 = context_start + token_pos\n",
    "        for i in range(char_start, char_end+1):\n",
    "            character_pos_to_token_pos[i] = token_pos1\n",
    "            \n",
    "    start_pos = charcter_pos_to_token_pos.get(answer_start_char, 0)\n",
    "    end_pos = charcter_pos_to_token_pos.get(\n",
    "        answer_end_char - 1, \n",
    "        0 if start_pos == 0 else config['max_length'] - 1\n",
    "    )\n",
    "        \n",
    "    inputs[\"start_positions\"] = start_pos\n",
    "    inputs[\"end_positions\"] = end_pos\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.920753Z",
     "iopub.status.busy": "2024-09-01T15:32:09.919030Z",
     "iopub.status.idle": "2024-09-01T15:32:09.933169Z",
     "shell.execute_reply": "2024-09-01T15:32:09.932112Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.920703Z"
    }
   },
   "outputs": [],
   "source": [
    "train, valid = model_selection.train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.938218Z",
     "iopub.status.busy": "2024-09-01T15:32:09.937799Z",
     "iopub.status.idle": "2024-09-01T15:32:09.943381Z",
     "shell.execute_reply": "2024-09-01T15:32:09.942242Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.938178Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = TextDataset(train)\n",
    "valid_ds = TextDataset(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:09.945446Z",
     "iopub.status.busy": "2024-09-01T15:32:09.944780Z",
     "iopub.status.idle": "2024-09-01T15:32:11.031700Z",
     "shell.execute_reply": "2024-09-01T15:32:11.030973Z",
     "shell.execute_reply.started": "2024-09-01T15:32:09.945397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ee3f42d59a4f72ab36add3a604250f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/51.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForQuestionAnswering.from_pretrained(config[\"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:11.033511Z",
     "iopub.status.busy": "2024-09-01T15:32:11.033162Z",
     "iopub.status.idle": "2024-09-01T15:32:11.692957Z",
     "shell.execute_reply": "2024-09-01T15:32:11.691933Z",
     "shell.execute_reply.started": "2024-09-01T15:32:11.033451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Load metrics\n",
    "    exact_match_metric = load_metric(\"exact_match\")\n",
    "    f1_metric = load_metric(\"f1\")\n",
    "\n",
    "    start_logits, end_logits = eval_pred.predictions\n",
    "    examples = eval_pred.label_ids\n",
    "    \n",
    "    # Extract information from examples\n",
    "    example_ids = examples[\"example_id\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    offset_mappings = examples[\"offset_mapping\"]\n",
    "    ground_truth_answers = examples[\"answers\"]  # Assume this contains the true answers\n",
    "    \n",
    "    exact_match = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    for i in range(len(example_ids)):\n",
    "        # Get the predicted start and end positions\n",
    "        start_logit = start_logits[i]\n",
    "        end_logit = end_logits[i]\n",
    "        \n",
    "        start_index = np.argmax(start_logit)\n",
    "        end_index = np.argmax(end_logit)\n",
    "        \n",
    "        # Get the offset mapping for this example\n",
    "        offsets = offset_mappings[i]\n",
    "        \n",
    "        # Get the predicted answer span\n",
    "        pred_start_char = offsets[start_index][0]\n",
    "        pred_end_char = offsets[end_index][1]\n",
    "        \n",
    "        pred_answer = contexts[i][pred_start_char:pred_end_char]\n",
    "        \n",
    "        # Get the ground truth answer\n",
    "        ground_truth_answer = ground_truth_answers[i]\n",
    "        \n",
    "        # Compute exact match\n",
    "        if pred_answer.strip() == ground_truth_answer.strip():\n",
    "            exact_match += 1\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1 += compute_f1(pred_answer, ground_truth_answer)\n",
    "    \n",
    "    total = len(example_ids)\n",
    "    avg_exact_match = exact_match / total\n",
    "    avg_f1 = f1 / total\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": avg_exact_match,\n",
    "        \"f1\": avg_f1\n",
    "    }\n",
    "\n",
    "def compute_f1(pred_answer, ground_truth_answer):\n",
    "    # Function to compute F1 score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    # Tokenize answers for F1 score calculation\n",
    "    pred_tokens = pred_answer.split()\n",
    "    ground_truth_tokens = ground_truth_answer.split()\n",
    "    \n",
    "    # Handle cases where there are no tokens\n",
    "    if not pred_tokens:\n",
    "        return 0 if ground_truth_tokens else 1\n",
    "    \n",
    "    if not ground_truth_tokens:\n",
    "        return 0\n",
    "    \n",
    "    return f1_score(ground_truth_tokens, pred_tokens, average=\"micro\")\n",
    "\n",
    "\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "     output_dir=\"./results\",                      # Directory for storing results\n",
    "    evaluation_strategy=\"steps\",                 # Evaluate every few steps\n",
    "    per_device_train_batch_size=config['train_batch_size'],              # Batch size per device during training\n",
    "    per_device_eval_batch_size=config['train_batch_size'],               # Batch size per device during evaluation\n",
    "    num_train_epochs=config['epochs'],                          # Total number of training epochs\n",
    "    warmup_steps=500,                            # Number of warmup steps for learning rate scheduler\n",
    "    save_total_limit=2,\n",
    "    logging_dir=None,                            # Disable logging directory\n",
    "    logging_strategy=\"no\",\n",
    "    report_to=[]# Limit the total amount of checkpoints`\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:11.695058Z",
     "iopub.status.busy": "2024-09-01T15:32:11.694376Z",
     "iopub.status.idle": "2024-09-01T15:32:27.011450Z",
     "shell.execute_reply": "2024-09-01T15:32:27.010432Z",
     "shell.execute_reply.started": "2024-09-01T15:32:11.695019Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,                                 # The model to be trained\n",
    "    args=training_args,                          # The training arguments, defined above\n",
    "    train_dataset=train_ds,                 # The training dataset\n",
    "    eval_dataset=valid_ds,                   # The evaluation dataset\n",
    "    tokenizer=tokenizer,                         # The tokenizer\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:32:27.013775Z",
     "iopub.status.busy": "2024-09-01T15:32:27.012898Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/900 06:47 < 05:25, 1.22 it/s, Epoch 166.67/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/2201463457.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  exact_match_metric = load_metric(\"exact_match\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d071f969daf5409c898cad061a7c8b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the QnA pipeline with a pre-trained model\n",
    "qna_pipeline = pipeline(\"question-answering\", model=\"microsoft/xtremedistil-l6-h256-uncased\")\n",
    "\n",
    "# Example input data\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country in Europe. The capital of France is Paris.\"\n",
    "\n",
    "# Perform inference\n",
    "result = qna_pipeline(question=question, context=context)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 374,
     "sourceId": 799923,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
